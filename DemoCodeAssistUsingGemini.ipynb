{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-27T07:36:51.709150Z",
     "start_time": "2025-08-27T07:36:51.703755Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "print('Start of code assist module')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of code assist module\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T09:11:09.534352Z",
     "start_time": "2025-08-27T09:10:13.827634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils import get_api_key\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from google.api_core import client_options as client_options_lib\n",
    "\n",
    "genai.configure(\n",
    "    api_key=get_api_key(),\n",
    "    transport='rest',\n",
    "    client_options=client_options_lib.ClientOptions(\n",
    "        api_endpoint=os.getenv(\"GOOGLE_API_BASE\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# for m in genai.list_models():\n",
    "#     print(f\"name: {m.name}\")\n",
    "#     print(f\"description: {m.description}\")\n",
    "#     print(f\"generation methods: {m.supported_generation_methods}\\n\")\n",
    "\n",
    "model_flash = genai.GenerativeModel(model_name='gemini-2.5-flash')\n",
    "def generate_text(\n",
    "        prompt,\n",
    "        model=model_flash,\n",
    "        temperature=0.0):\n",
    "    return model_flash.generate_content(prompt, generation_config={'temperature': temperature})\n",
    "\n",
    "prompt = \"How to troubleshoot out of memory error in python\"\n",
    "text = generate_text(prompt)\n",
    "print(text)\n",
    "\n",
    "completion = generate_text(prompt)\n",
    "\n",
    "print(completion.text)"
   ],
   "id": "beeb4316503518a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"Out of Memory (OOM) errors in Python can be frustrating, as they often indicate a deeper issue with how your program manages resources. Troubleshooting them involves a systematic approach of identifying, profiling, and optimizing memory usage.\\n\\nHere's a comprehensive guide to troubleshooting Python OOM errors:\\n\\n---\\n\\n## I. Initial Steps & Information Gathering\\n\\nBefore diving into profiling, gather some basic information:\\n\\n1.  **Understand the Error Message:**\\n    *   `MemoryError`: This is Python's built-in exception when it can't allocate memory for an object.\\n    *   `OSError: [Errno 12] Cannot allocate memory`: This often means the *operating system* itself is refusing to allocate more memory to your Python process, usually because the system is running low on RAM or swap space. This can happen even if Python's internal memory management isn't the direct cause.\\n    *   Other errors (e.g., from NumPy, Pandas, TensorFlow): These libraries might have their own memory allocation mechanisms that fail.\\n\\n2.  **When Does it Happen?**\\n    *   **Immediately on startup?** Might be a large initial data load or a configuration issue.\\n    *   **After running for a while?** Suggests a memory leak or gradual accumulation of data.\\n    *   **During a specific function/operation?** Pinpoints the problematic code section.\\n    *   **With specific input data?** Indicates scalability issues with large datasets.\\n\\n3.  **Check System Resources:**\\n    *   **Linux/macOS:** Use `free -h`, `top`, `htop`, `glances` to see overall system memory usage, swap usage, and individual process memory.\\n    *   **Windows:** Use Task Manager (Performance tab, then Processes tab).\\n    *   Is your Python process consuming *all* available RAM, or is the system already under heavy load from other applications?\\n\\n4.  **Simplify/Isolate the Problem:**\\n    *   Can you create a minimal reproducible example?\\n    *   Can you run the script with a smaller dataset? If it works, the problem is likely related to data volume.\\n\\n---\\n\\n## II. Memory Profiling & Analysis (The Core)\\n\\nThis is where you identify *what* is consuming memory.\\n\\n### 1. Basic Python Tools\\n\\n*   **`sys.getsizeof(obj)`:** Returns the size of an object in bytes. Useful for individual objects, but doesn't account for objects referenced by `obj`.\\n    ```python\\n    import sys\\n    my_list = [i for i in range(100000)]\\n    print(f\\\"Size of list: {sys.getsizeof(my_list)} bytes\\\")\\n    my_dict = {i: str(i) for i in range(100000)}\\n    print(f\\\"Size of dict: {sys.getsizeof(my_dict)} bytes\\\")\\n    ```\\n*   **`gc.get_objects()`:** Returns a list of all objects tracked by the garbage collector. Can be overwhelming, but useful for advanced debugging, especially with `objgraph`.\\n*   **`tracemalloc` (Python 3.4+):** Built-in module to trace memory allocations. It can show you where memory blocks were allocated and their size.\\n    ```python\\n    import tracemalloc\\n\\n    tracemalloc.start()\\n\\n    # ... your code that allocates memory ...\\n    data = [b'x' * 1024 for _ in range(1000)] # Allocate 1MB * 1000 = 1GB\\n\\n    snapshot = tracemalloc.take_snapshot()\\n    top_stats = snapshot.statistics('lineno')\\n\\n    print(\\\"[ Top 10 ]\\\")\\n    for stat in top_stats[:10]:\\n        print(stat)\\n\\n    tracemalloc.stop()\\n    ```\\n    This will show you the file and line number where the most memory was allocated.\\n\\n### 2. Dedicated Memory Profilers (Recommended)\\n\\nThese tools provide much more detailed insights.\\n\\n*   **`memory_profiler`:** A line-by-line memory profiler. It's excellent for pinpointing exactly which lines of code are consuming memory.\\n    1.  **Install:** `pip install memory_profiler`\\n    2.  **Usage:**\\n        *   **Decorator:** Add `@profile` to functions you want to monitor.\\n            ```python\\n            # my_script.py\\n            @profile\\n            def process_data(data_size):\\n                a = [0] * data_size  # This line will show memory usage\\n                b = [1] * data_size\\n                del a # Memory for 'a' is released\\n                return b\\n\\n            if __name__ == '__main__':\\n                process_data(10**7) # Allocate 10 million integers\\n            ```\\n        *   **Run:** `python -m memory_profiler my_script.py`\\n        *   **Output:** Shows memory usage (MiB) for each line within the profiled functions.\\n\\n*   **`Pympler`:** Provides a set of tools to measure the size of Python objects, analyze memory usage, and detect memory leaks.\\n    1.  **Install:** `pip install Pympler`\\n    2.  **Usage:**\\n        *   **`asizeof`:** More accurate size calculation than `sys.getsizeof` as it accounts for referenced objects.\\n            ```python\\n            from pympler import asizeof\\n            my_list = [i for i in range(100000)]\\n            print(f\\\"Pympler size of list: {asizeof.asizeof(my_list)} bytes\\\")\\n            ```\\n        *   **`muppy`:** Tracks all live objects.\\n            ```python\\n            from pympler import muppy, summary\\n            all_objects = muppy.get_objects()\\n            sum1 = summary.summarize(all_objects)\\n            summary.print_(sum1)\\n\\n            # Do some memory-intensive work\\n            data = [str(i) * 100 for i in range(100000)]\\n\\n            all_objects = muppy.get_objects()\\n            sum2 = summary.summarize(all_objects)\\n            # Compare snapshots to find new objects\\n            summary.print_(summary.diff(sum1, sum2))\\n            ```\\n        *   **`refbrowser`:** Helps visualize object references, useful for finding circular references and leaks.\\n\\n*   **`objgraph`:** Visualizes object references and can help identify memory leaks by showing what's holding onto objects.\\n    1.  **Install:** `pip install objgraph`\\n    2.  **Usage:**\\n        ```python\\n        import objgraph\\n        # Create some objects\\n        a = []\\n        b = [a]\\n        c = [b]\\n        a.append(c) # Circular reference\\n\\n        # Show common types\\n        objgraph.show_most_common_types(limit=10)\\n\\n        # Show objects of a specific type\\n        # objgraph.show_backrefs(a, filename='a_backrefs.png')\\n        # objgraph.show_chain(objgraph.find_backref_chain(a, objgraph.is_proper_module), filename='chain.png')\\n        ```\\n        `objgraph` often requires `graphviz` to be installed on your system for generating images.\\n\\n---\\n\\n## III. Common Causes & Solutions\\n\\nOnce you've identified *what* is consuming memory, here are common causes and their solutions:\\n\\n### 1. Large Data Structures\\n\\n*   **Problem:** Loading entire large files, datasets (e.g., CSVs, JSONs), or creating huge lists/dictionaries/NumPy arrays/Pandas DataFrames directly into memory.\\n*   **Solutions:**\\n    *   **Iterators & Generators:** Process data in chunks or on-the-fly. Use `yield` for custom generators.\\n        ```python\\n        # Instead of:\\n        # all_lines = file.readlines()\\n        # for line in all_lines: ...\\n\\n        # Use:\\n        def read_large_file(filepath):\\n            with open(filepath, 'r') as f:\\n                for line in f:\\n                    yield line.strip()\\n\\n        for line in read_large_file('large_file.txt'):\\n            # Process line\\n            pass\\n        ```\\n    *   **`itertools`:** Provides efficient iterator-based tools (e.g., `islice`, `chain`).\\n    *   **Pandas `chunksize`:** For large CSVs, read in chunks.\\n        ```python\\n        import pandas as pd\\n        for chunk in pd.read_csv('large_data.csv', chunksize=10000):\\n            # Process each chunk (DataFrame)\\n            pass\\n        ```\\n    *   **NumPy `memmap`:** Memory-map large arrays to disk, loading only parts into RAM as needed.\\n    *   **Specialized Libraries:** For truly massive datasets, consider:\\n        *   **Dask:** Parallel computing library that can handle larger-than-memory datasets (DataFrames, Arrays).\\n        *   **Vaex / Polars:** High-performance DataFrame libraries optimized for large datasets, often using memory-mapping or efficient columnar storage.\\n    *   **Efficient Data Types:**\\n        *   **NumPy/Pandas:** Use smaller integer types (`int8`, `int16`), float types (`float32`), or Pandas `category` dtype for columns with limited unique values.\\n        *   **Python:** Avoid storing large numbers as strings if they can be integers.\\n\\n### 2. Unnecessary Data Duplication\\n\\n*   **Problem:** Creating copies of large objects when a view or in-place modification would suffice. Common with list slicing, `df.copy()`, or repeatedly appending to lists in loops (which can cause reallocations).\\n*   **Solutions:**\\n    *   **In-place operations:** Modify objects directly where possible.\\n    *   **Views:** Use slices that return views (e.g., NumPy array slices often return views, but be careful with Pandas).\\n    *   **List Appending:** If appending many items, consider pre-allocating a list or using a generator expression with `list()` if the final size is known.\\n    *   **`del` and `gc.collect()`:** Explicitly delete large objects when they are no longer needed, especially in long-running processes. `gc.collect()` forces a garbage collection cycle, but Python's GC is usually efficient enough.\\n\\n### 3. Memory Leaks\\n\\n*   **Problem:** Objects are no longer needed but are still referenced, preventing the garbage collector from reclaiming their memory.\\n*   **Common Causes:**\\n    *   **Circular References:** Two or more objects reference each other, forming a cycle. Python's GC can usually handle these, but not always (especially with objects having `__del__` methods or C extensions).\\n        *   **Solution:** Use `weakref` to break cycles where appropriate.\\n    *   **Global Variables:** Large objects stored in global variables persist for the life of the program.\\n    *   **Closures:** Functions that \\\"capture\\\" variables from their enclosing scope can inadvertently hold onto large objects.\\n    *   **C Extensions:** Libraries written in C (like parts of NumPy, TensorFlow) might have their own memory management and can leak if not handled correctly.\\n    *   **Event Listeners/Callbacks:** If you register a callback that holds a strong reference to an object, and the listener itself isn't properly unregistered, it can lead to a leak.\\n*   **Tools for Detection:** `objgraph`, `Pympler`, `tracemalloc`.\\n\\n### 4. Excessive Recursion Depth\\n\\n*   **Problem:** Deep recursive calls consume stack memory for each frame. While usually leading to `RecursionError`, extreme cases can contribute to OOM.\\n*   **Solutions:**\\n    *   **Iterative Solutions:** Convert recursive algorithms to iterative ones (e.g., using a stack data structure).\\n    *   **`sys.setrecursionlimit()`:** Increase the limit (use with caution, as it can lead to stack overflow if not managed).\\n\\n### 5. External Libraries & C Extensions\\n\\n*   **Problem:** Libraries like NumPy, Pandas, TensorFlow, PyTorch, OpenCV, etc., often allocate memory outside of Python's direct control (e.g., in C/C++ heaps). Python's `sys.getsizeof` won't reflect this.\\n*   **Solutions:**\\n    *   **Check Library Documentation:** Understand their memory management.\\n    *   **Batch Processing:** For ML models, use smaller batch sizes.\\n    *   **Model Size:** Use smaller models or quantize models if possible.\\n    *   **GPU Offloading:** If applicable, offload computations and data to a GPU.\\n    *   **Explicitly Clear:** After using large objects from these libraries, `del` them and potentially call `gc.collect()` (though the latter is often not strictly necessary for C-allocated memory).\\n    *   **Context Managers:** Use `with` statements for resources that need explicit cleanup.\\n\\n### 6. File I/O\\n\\n*   **Problem:** Reading entire large files into memory at once.\\n*   **Solutions:**\\n    *   **Read Line by Line:** `for line in file_object:`\\n    *   **Read in Chunks:** `file_object.read(chunk_size)`\\n    *   **Memory Mapping (`mmap`):** For very large files, `mmap` can map the file directly into the process's virtual address space, allowing you to access parts of it as if it were in memory without loading the whole thing.\\n\\n### 7. System-Wide Memory Pressure\\n\\n*   **Problem:** Your Python script might not be the *only* memory hog. Other applications, services, or even the OS itself might be consuming most of the available RAM.\\n*   **Solutions:**\\n    *   **Close Other Applications:** Free up RAM.\\n    *   **Increase RAM:** The simplest (but not always feasible) solution.\\n    *   **Run on Dedicated Machine/Container:** Isolate your application's memory usage.\\n    *   **Container Resource Limits:** If using Docker/Kubernetes, set memory limits for your containers.\\n\\n---\\n\\n## IV. Best Practices for Prevention\\n\\n*   **Modular Code:** Break down complex tasks into smaller functions.\\n*   **Test with Realistic Data:** Don't just test with small samples; use data sizes that reflect production.\\n*   **Code Reviews:** A fresh pair of eyes can spot potential memory issues.\\n*   **Understand Data Structures:** Choose the right data structure for the job (e.g., `set` for fast lookups, `tuple` for immutable sequences, generators for lazy evaluation).\\n*   **Profile Regularly:** Integrate memory profiling into your development workflow, especially for performance-critical parts.\\n\\n---\\n\\nTroubleshooting OOM errors is an iterative process. Start with basic checks, then use profiling tools to pinpoint the exact cause, and finally apply the appropriate optimization techniques. Good luck!\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 10,\n",
      "        \"candidates_token_count\": 3335,\n",
      "        \"total_token_count\": 4949\n",
      "      },\n",
      "      \"model_version\": \"gemini-2.5-flash\"\n",
      "    }),\n",
      ")\n",
      "Out of Memory (OOM) errors in Python can be frustrating, as they often indicate a deeper issue with how your program manages resources. Troubleshooting them involves a systematic approach of identifying, profiling, and optimizing memory usage.\n",
      "\n",
      "Here's a comprehensive guide to troubleshooting Python OOM errors:\n",
      "\n",
      "---\n",
      "\n",
      "## I. Initial Steps & Information Gathering\n",
      "\n",
      "Before diving into profiling, gather some basic information:\n",
      "\n",
      "1.  **Understand the Error Message:**\n",
      "    *   `MemoryError`: This is Python's built-in exception when it can't allocate memory for an object.\n",
      "    *   `OSError: [Errno 12] Cannot allocate memory`: This often means the *operating system* itself is refusing to allocate more memory to your Python process, usually because the system is running low on RAM or swap space. This can happen even if Python's internal memory management isn't the direct cause.\n",
      "    *   Other errors (e.g., from NumPy, Pandas, TensorFlow): These libraries might have their own memory allocation mechanisms that fail.\n",
      "\n",
      "2.  **When Does it Happen?**\n",
      "    *   **Immediately on startup?** Might be a large initial data load or a configuration issue.\n",
      "    *   **After running for a while?** Suggests a memory leak or gradual accumulation of data.\n",
      "    *   **During a specific function/operation?** Pinpoints the problematic code section.\n",
      "    *   **With specific input data?** Indicates scalability issues with large datasets.\n",
      "\n",
      "3.  **Check System Resources:**\n",
      "    *   **Linux/macOS:** Use `free -h`, `top`, `htop`, `glances` to see overall system memory usage, swap usage, and individual process memory.\n",
      "    *   **Windows:** Use Task Manager (Performance tab, then Processes tab).\n",
      "    *   Is your Python process consuming *all* available RAM, or is the system already under heavy load from other applications?\n",
      "\n",
      "4.  **Simplify/Isolate the Problem:**\n",
      "    *   Can you create a minimal reproducible example?\n",
      "    *   Can you run the script with a smaller dataset? If it works, the problem is likely related to data volume.\n",
      "\n",
      "---\n",
      "\n",
      "## II. Memory Profiling & Analysis (The Core)\n",
      "\n",
      "This is where you identify *what* is consuming memory.\n",
      "\n",
      "### 1. Basic Python Tools\n",
      "\n",
      "*   **`sys.getsizeof(obj)`:** Returns the size of an object in bytes. Useful for individual objects, but doesn't account for objects referenced by `obj`.\n",
      "    ```python\n",
      "    import sys\n",
      "    my_list = [i for i in range(100000)]\n",
      "    print(f\"Size of list: {sys.getsizeof(my_list)} bytes\")\n",
      "    my_dict = {i: str(i) for i in range(100000)}\n",
      "    print(f\"Size of dict: {sys.getsizeof(my_dict)} bytes\")\n",
      "    ```\n",
      "*   **`gc.get_objects()`:** Returns a list of all objects tracked by the garbage collector. Can be overwhelming, but useful for advanced debugging, especially with `objgraph`.\n",
      "*   **`tracemalloc` (Python 3.4+):** Built-in module to trace memory allocations. It can show you where memory blocks were allocated and their size.\n",
      "    ```python\n",
      "    import tracemalloc\n",
      "\n",
      "    tracemalloc.start()\n",
      "\n",
      "    # ... your code that allocates memory ...\n",
      "    data = [b'x' * 1024 for _ in range(1000)] # Allocate 1MB * 1000 = 1GB\n",
      "\n",
      "    snapshot = tracemalloc.take_snapshot()\n",
      "    top_stats = snapshot.statistics('lineno')\n",
      "\n",
      "    print(\"[ Top 10 ]\")\n",
      "    for stat in top_stats[:10]:\n",
      "        print(stat)\n",
      "\n",
      "    tracemalloc.stop()\n",
      "    ```\n",
      "    This will show you the file and line number where the most memory was allocated.\n",
      "\n",
      "### 2. Dedicated Memory Profilers (Recommended)\n",
      "\n",
      "These tools provide much more detailed insights.\n",
      "\n",
      "*   **`memory_profiler`:** A line-by-line memory profiler. It's excellent for pinpointing exactly which lines of code are consuming memory.\n",
      "    1.  **Install:** `pip install memory_profiler`\n",
      "    2.  **Usage:**\n",
      "        *   **Decorator:** Add `@profile` to functions you want to monitor.\n",
      "            ```python\n",
      "            # my_script.py\n",
      "            @profile\n",
      "            def process_data(data_size):\n",
      "                a = [0] * data_size  # This line will show memory usage\n",
      "                b = [1] * data_size\n",
      "                del a # Memory for 'a' is released\n",
      "                return b\n",
      "\n",
      "            if __name__ == '__main__':\n",
      "                process_data(10**7) # Allocate 10 million integers\n",
      "            ```\n",
      "        *   **Run:** `python -m memory_profiler my_script.py`\n",
      "        *   **Output:** Shows memory usage (MiB) for each line within the profiled functions.\n",
      "\n",
      "*   **`Pympler`:** Provides a set of tools to measure the size of Python objects, analyze memory usage, and detect memory leaks.\n",
      "    1.  **Install:** `pip install Pympler`\n",
      "    2.  **Usage:**\n",
      "        *   **`asizeof`:** More accurate size calculation than `sys.getsizeof` as it accounts for referenced objects.\n",
      "            ```python\n",
      "            from pympler import asizeof\n",
      "            my_list = [i for i in range(100000)]\n",
      "            print(f\"Pympler size of list: {asizeof.asizeof(my_list)} bytes\")\n",
      "            ```\n",
      "        *   **`muppy`:** Tracks all live objects.\n",
      "            ```python\n",
      "            from pympler import muppy, summary\n",
      "            all_objects = muppy.get_objects()\n",
      "            sum1 = summary.summarize(all_objects)\n",
      "            summary.print_(sum1)\n",
      "\n",
      "            # Do some memory-intensive work\n",
      "            data = [str(i) * 100 for i in range(100000)]\n",
      "\n",
      "            all_objects = muppy.get_objects()\n",
      "            sum2 = summary.summarize(all_objects)\n",
      "            # Compare snapshots to find new objects\n",
      "            summary.print_(summary.diff(sum1, sum2))\n",
      "            ```\n",
      "        *   **`refbrowser`:** Helps visualize object references, useful for finding circular references and leaks.\n",
      "\n",
      "*   **`objgraph`:** Visualizes object references and can help identify memory leaks by showing what's holding onto objects.\n",
      "    1.  **Install:** `pip install objgraph`\n",
      "    2.  **Usage:**\n",
      "        ```python\n",
      "        import objgraph\n",
      "        # Create some objects\n",
      "        a = []\n",
      "        b = [a]\n",
      "        c = [b]\n",
      "        a.append(c) # Circular reference\n",
      "\n",
      "        # Show common types\n",
      "        objgraph.show_most_common_types(limit=10)\n",
      "\n",
      "        # Show objects of a specific type\n",
      "        # objgraph.show_backrefs(a, filename='a_backrefs.png')\n",
      "        # objgraph.show_chain(objgraph.find_backref_chain(a, objgraph.is_proper_module), filename='chain.png')\n",
      "        ```\n",
      "        `objgraph` often requires `graphviz` to be installed on your system for generating images.\n",
      "\n",
      "---\n",
      "\n",
      "## III. Common Causes & Solutions\n",
      "\n",
      "Once you've identified *what* is consuming memory, here are common causes and their solutions:\n",
      "\n",
      "### 1. Large Data Structures\n",
      "\n",
      "*   **Problem:** Loading entire large files, datasets (e.g., CSVs, JSONs), or creating huge lists/dictionaries/NumPy arrays/Pandas DataFrames directly into memory.\n",
      "*   **Solutions:**\n",
      "    *   **Iterators & Generators:** Process data in chunks or on-the-fly. Use `yield` for custom generators.\n",
      "        ```python\n",
      "        # Instead of:\n",
      "        # all_lines = file.readlines()\n",
      "        # for line in all_lines: ...\n",
      "\n",
      "        # Use:\n",
      "        def read_large_file(filepath):\n",
      "            with open(filepath, 'r') as f:\n",
      "                for line in f:\n",
      "                    yield line.strip()\n",
      "\n",
      "        for line in read_large_file('large_file.txt'):\n",
      "            # Process line\n",
      "            pass\n",
      "        ```\n",
      "    *   **`itertools`:** Provides efficient iterator-based tools (e.g., `islice`, `chain`).\n",
      "    *   **Pandas `chunksize`:** For large CSVs, read in chunks.\n",
      "        ```python\n",
      "        import pandas as pd\n",
      "        for chunk in pd.read_csv('large_data.csv', chunksize=10000):\n",
      "            # Process each chunk (DataFrame)\n",
      "            pass\n",
      "        ```\n",
      "    *   **NumPy `memmap`:** Memory-map large arrays to disk, loading only parts into RAM as needed.\n",
      "    *   **Specialized Libraries:** For truly massive datasets, consider:\n",
      "        *   **Dask:** Parallel computing library that can handle larger-than-memory datasets (DataFrames, Arrays).\n",
      "        *   **Vaex / Polars:** High-performance DataFrame libraries optimized for large datasets, often using memory-mapping or efficient columnar storage.\n",
      "    *   **Efficient Data Types:**\n",
      "        *   **NumPy/Pandas:** Use smaller integer types (`int8`, `int16`), float types (`float32`), or Pandas `category` dtype for columns with limited unique values.\n",
      "        *   **Python:** Avoid storing large numbers as strings if they can be integers.\n",
      "\n",
      "### 2. Unnecessary Data Duplication\n",
      "\n",
      "*   **Problem:** Creating copies of large objects when a view or in-place modification would suffice. Common with list slicing, `df.copy()`, or repeatedly appending to lists in loops (which can cause reallocations).\n",
      "*   **Solutions:**\n",
      "    *   **In-place operations:** Modify objects directly where possible.\n",
      "    *   **Views:** Use slices that return views (e.g., NumPy array slices often return views, but be careful with Pandas).\n",
      "    *   **List Appending:** If appending many items, consider pre-allocating a list or using a generator expression with `list()` if the final size is known.\n",
      "    *   **`del` and `gc.collect()`:** Explicitly delete large objects when they are no longer needed, especially in long-running processes. `gc.collect()` forces a garbage collection cycle, but Python's GC is usually efficient enough.\n",
      "\n",
      "### 3. Memory Leaks\n",
      "\n",
      "*   **Problem:** Objects are no longer needed but are still referenced, preventing the garbage collector from reclaiming their memory.\n",
      "*   **Common Causes:**\n",
      "    *   **Circular References:** Two or more objects reference each other, forming a cycle. Python's GC can usually handle these, but not always (especially with objects having `__del__` methods or C extensions).\n",
      "        *   **Solution:** Use `weakref` to break cycles where appropriate.\n",
      "    *   **Global Variables:** Large objects stored in global variables persist for the life of the program.\n",
      "    *   **Closures:** Functions that \"capture\" variables from their enclosing scope can inadvertently hold onto large objects.\n",
      "    *   **C Extensions:** Libraries written in C (like parts of NumPy, TensorFlow) might have their own memory management and can leak if not handled correctly.\n",
      "    *   **Event Listeners/Callbacks:** If you register a callback that holds a strong reference to an object, and the listener itself isn't properly unregistered, it can lead to a leak.\n",
      "*   **Tools for Detection:** `objgraph`, `Pympler`, `tracemalloc`.\n",
      "\n",
      "### 4. Excessive Recursion Depth\n",
      "\n",
      "*   **Problem:** Deep recursive calls consume stack memory for each frame. While usually leading to `RecursionError`, extreme cases can contribute to OOM.\n",
      "*   **Solutions:**\n",
      "    *   **Iterative Solutions:** Convert recursive algorithms to iterative ones (e.g., using a stack data structure).\n",
      "    *   **`sys.setrecursionlimit()`:** Increase the limit (use with caution, as it can lead to stack overflow if not managed).\n",
      "\n",
      "### 5. External Libraries & C Extensions\n",
      "\n",
      "*   **Problem:** Libraries like NumPy, Pandas, TensorFlow, PyTorch, OpenCV, etc., often allocate memory outside of Python's direct control (e.g., in C/C++ heaps). Python's `sys.getsizeof` won't reflect this.\n",
      "*   **Solutions:**\n",
      "    *   **Check Library Documentation:** Understand their memory management.\n",
      "    *   **Batch Processing:** For ML models, use smaller batch sizes.\n",
      "    *   **Model Size:** Use smaller models or quantize models if possible.\n",
      "    *   **GPU Offloading:** If applicable, offload computations and data to a GPU.\n",
      "    *   **Explicitly Clear:** After using large objects from these libraries, `del` them and potentially call `gc.collect()` (though the latter is often not strictly necessary for C-allocated memory).\n",
      "    *   **Context Managers:** Use `with` statements for resources that need explicit cleanup.\n",
      "\n",
      "### 6. File I/O\n",
      "\n",
      "*   **Problem:** Reading entire large files into memory at once.\n",
      "*   **Solutions:**\n",
      "    *   **Read Line by Line:** `for line in file_object:`\n",
      "    *   **Read in Chunks:** `file_object.read(chunk_size)`\n",
      "    *   **Memory Mapping (`mmap`):** For very large files, `mmap` can map the file directly into the process's virtual address space, allowing you to access parts of it as if it were in memory without loading the whole thing.\n",
      "\n",
      "### 7. System-Wide Memory Pressure\n",
      "\n",
      "*   **Problem:** Your Python script might not be the *only* memory hog. Other applications, services, or even the OS itself might be consuming most of the available RAM.\n",
      "*   **Solutions:**\n",
      "    *   **Close Other Applications:** Free up RAM.\n",
      "    *   **Increase RAM:** The simplest (but not always feasible) solution.\n",
      "    *   **Run on Dedicated Machine/Container:** Isolate your application's memory usage.\n",
      "    *   **Container Resource Limits:** If using Docker/Kubernetes, set memory limits for your containers.\n",
      "\n",
      "---\n",
      "\n",
      "## IV. Best Practices for Prevention\n",
      "\n",
      "*   **Modular Code:** Break down complex tasks into smaller functions.\n",
      "*   **Test with Realistic Data:** Don't just test with small samples; use data sizes that reflect production.\n",
      "*   **Code Reviews:** A fresh pair of eyes can spot potential memory issues.\n",
      "*   **Understand Data Structures:** Choose the right data structure for the job (e.g., `set` for fast lookups, `tuple` for immutable sequences, generators for lazy evaluation).\n",
      "*   **Profile Regularly:** Integrate memory profiling into your development workflow, especially for performance-critical parts.\n",
      "\n",
      "---\n",
      "\n",
      "Troubleshooting OOM errors is an iterative process. Start with basic checks, then use profiling tools to pinpoint the exact cause, and finally apply the appropriate optimization techniques. Good luck!\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
